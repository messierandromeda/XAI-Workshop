{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Opening the Black Box with LRP: A Hands-on Guide to Explainable AI for LLMs\n",
    "\n",
    " ### by Reduan Achtibat & Anton Segeler\n",
    "\n",
    "**Explainable AI Group at Fraunhofer HHI**\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "We start by installing and importing the necessary libraries. We also set up the environment variables.\n",
    "\n"
   ],
   "id": "5e1dc2092c91de4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run this cell only when using Colab\n",
    "!git clone https://github.com/antonsegeler/xai4llms\n",
    "!cp -r xai4llms/. ."
   ],
   "id": "f84d4312327a3fb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# installing packages with pip\n",
    "%pip install -r requirements.txt"
   ],
   "id": "61af3cda06e13199"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama import modeling_llama\n",
    "import gradio as gr\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jscatter import Scatter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "%matplotlib inline\n",
    "\n",
    "from utils.plotting import decode_tokens_for_plotting, jupyter_heatmap, plot_atlas"
   ],
   "id": "e50d5ecf626e4324"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduction: The Transformer Brain",
   "id": "6b66601764991eab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "HTML('assets/attention_viz.html') # If the visualization disappears, close and reopen the notebook",
   "id": "fc0d1b0a099a7ea7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.mps.is_available() else \"cpu\"))\n",
    "\n",
    "path = 'unsloth/Llama-3.2-1B-Instruct'\n",
    "\n",
    "model = modeling_llama.LlamaForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    device_map=device,\n",
    "    dtype=torch.bfloat16, # to prevent numeric overflow! (float32/bfloat16 vs float16)\n",
    "    attn_implementation=\"eager\", # we need the attn weights!\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ],
   "id": "988b7ba26a518fba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we are doing inference and attribution (not training), we freeze the model parameters to save memory and computation.",
   "id": "ce2ed27a176343a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Deactivate gradients on parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "id": "27150ec7422a2821"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prologue: Building Our Microscope",
   "id": "f8d062a1038b1803"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß™ Experiment 1: Naive Perturbation\n",
   "id": "966c65c3b12c9bf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the prompt\n",
    "prompt = \"\"\"\\\n",
    "NASA's CALIPSO satellite measured 182 million tons of Sahara dust annually, but recent studies suggest it's actually 27 million tons.\n",
    "Q: How much dust is blown from the Sahara each year?\n",
    "A: \"\"\""
   ],
   "id": "65140f8fe54b99cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This function performs occlusion. It masks one token at a time and measures how much the probability of the target prediction drops. A large drop means the masked token was important.",
   "id": "70551b498f796527"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.no_grad()\n",
    "def attribute_perturbation(model, tokenizer, prompt, baseline_token='_', target=None):\n",
    "    \"\"\"\n",
    "    Perturbation attribution adapted for Causal LLMs (Llama, Mistral, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Prepare Input (Shape: [1, Sequence_Length])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    # Get the integer ID for the replacement token (underscore)\n",
    "    # add_special_tokens=False ensures we get just the ID, not [BOS, ID]\n",
    "    underscore_id = tokenizer(baseline_token, add_special_tokens=False).input_ids[0]\n",
    "\n",
    "    # 2. Get Baseline Prediction (Original Logits)\n",
    "    base_outputs = model(input_ids, use_cache=False)\n",
    "    base_logits = base_outputs.logits # Shape: [1, Seq_Len, Vocab_Size]\n",
    "\n",
    "    # We only care about the logits for the FINAL token (prediction of the next word)\n",
    "    next_token_logits = base_logits[0, -1, :]\n",
    "\n",
    "    # If no target provided, pick the model's top prediction\n",
    "    if target is None:\n",
    "        target = torch.argmax(next_token_logits).item()\n",
    "\n",
    "    base_score = next_token_logits[target].item()\n",
    "\n",
    "    # 3. Prepare Attribution Loop\n",
    "    seq_len = input_ids.shape[1]\n",
    "    attr = torch.zeros(seq_len)\n",
    "\n",
    "    print(f\"Target Token: '{tokenizer.decode([target])}' | Analyzing {seq_len} tokens...\")\n",
    "\n",
    "    # 4. Occlusion Loop\n",
    "    for i in range(seq_len):\n",
    "        # Create a copy of input_ids\n",
    "        input_ids_occ = input_ids.clone()\n",
    "\n",
    "        # Replace the i-th token with the underscore ID\n",
    "        # ....\n",
    "\n",
    "        # Run model on occluded input\n",
    "        # ....\n",
    "\n",
    "        # Get score of the SAME target token at the LAST position\n",
    "        # ....\n",
    "\n",
    "        # Attribution = How much did the score DROP when we removed this token?\n",
    "        # (High positive value = Token was very important)\n",
    "        # ....\n",
    "        raise NotImplementedError(\"Complete the occlusion loop steps marked with '....'\")\n",
    "\n",
    "    # Normalize between [-1, 1]\n",
    "    max_val = attr.abs().max()\n",
    "\n",
    "    # Epsilon prevents division by zero if all scores are identical\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    attr = attr / (max_val + epsilon)\n",
    "\n",
    "    # Return attribution scores and the token ID that was targeted\n",
    "    return attr.detach().cpu().numpy(), target"
   ],
   "id": "4cf1af91ea2cf868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Run the occlusion\n",
    "pert_relevances, pert_target = attribute_perturbation(model, tokenizer, prompt)\n",
    "\n",
    "# 2. Get the input IDs to decode them one by one\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
    "tokens = decode_tokens_for_plotting(input_ids, tokenizer)\n",
    "jupyter_heatmap(tokens, pert_relevances, cmap='bwr')"
   ],
   "id": "5c0ce70cb4c95743"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The Gradient Shortcut\n",
   "id": "a0f3aced9fda0415"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß™ Experiment 2: First-Order Taylor Approximation\n",
   "id": "7cf3170f17fea94e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instead of masking tokens one by one (which is slow), we can use Input √ó Gradient. By computing the gradient of the target prediction with respect to the input embeddings, we measure how sensitive the model is to each specific token.",
   "id": "51c787a49f9c8171"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def attribute_input_x_gradient(model, tokenizer, prompt, target=None):\n",
    "    \"\"\"\n",
    "    Computes Input * Gradient attribution for LLMs.\n",
    "    Actually computes: DotProduct(Embedding, Gradient w.r.t Embedding)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Prepare Input and Embeddings\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    # We must access embeddings directly to track gradients on them\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    input_embeds = embedding_layer(input_ids).detach()\n",
    "    input_embeds.requires_grad = True # CRITICAL: Enable gradient tracking\n",
    "\n",
    "    # 2. Forward Pass\n",
    "    outputs = model(inputs_embeds=input_embeds, use_cache=False)\n",
    "\n",
    "    # Select target (Prediction of the last token)\n",
    "    if target is None:\n",
    "        target = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "\n",
    "    # 3. Get the target score\n",
    "    # ....\n",
    "\n",
    "    # 4. Backward Pass\n",
    "    # ....\n",
    "\n",
    "    # Gradients w.r.t the embeddings [1, Seq_Len, Hidden_Dim]\n",
    "    # ....\n",
    "\n",
    "    # 5. Input * Gradient Calculation\n",
    "    # Element-wise multiplication followed by sum over the last hidden dimension\n",
    "    # resulting in one scalar score per token.\n",
    "    # output shape [batch_size, Seq_Len, Hidden_Dim]\n",
    "    # ....\n",
    "    raise NotImplementedError(\"Complete the Input x Gradient steps marked with '....'\")\n",
    "\n",
    "\n",
    "    # Normalize between [-1, 1]\n",
    "    max_val = attr.abs().max()\n",
    "\n",
    "    # Epsilon prevents division by zero if all scores are identical\n",
    "    epsilon = 1e-9\n",
    "\n",
    "    attr = attr / (max_val + epsilon)\n",
    "\n",
    "    # Return attribution scores and the token ID that was targeted\n",
    "    return attr.detach().cpu().numpy(), target"
   ],
   "id": "78964bdae28742e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run Input x Gradient\n",
    "ixg_relevances, ixg_target = attribute_input_x_gradient(model, tokenizer, prompt)\n",
    "\n",
    "jupyter_heatmap(tokens, ixg_relevances, cmap='bwr')"
   ],
   "id": "5c8661fa146f08b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The Problem: Gradient Shattering\n",
   "id": "a2ad6c02063dc2bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " While Gratient Analysis is faster, raw gradients can be \"noisy\" or shattered.",
   "id": "c753d5737a209653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "HTML('assets/gradient_noise.html')",
   "id": "6272725ad4e5b1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Removing the Noise with LRP",
   "id": "c44cacd35d3bbddc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß™ Experiment 3: Layer-wise Relevance Propagation\n",
   "id": "fca2e9d75b647fbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_Monkey Patching_ is a technique in dynamic programming languages (such as Python) for modifying or extending the behavior of modules, classes, or objects at runtime without changing the original source code.",
   "id": "e09d1ca03ea3d76c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modify the LLaMA module to compute LRP in the backward pass\n",
    "from lxt.efficient import monkey_patch\n",
    "from lxt.utils import clean_tokens\n",
    "\n",
    "monkey_patch(modeling_llama, verbose=True)"
   ],
   "id": "33d9d4dbbbd97b49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Thanks to the LRP-patch, the resulting heatmap should look cleaner and more semantically meaningful than the raw gradients.",
   "id": "c3d95c5b192f4a43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run LRP\n",
    "lrp_relevances, lrp_target = attribute_input_x_gradient(model, tokenizer, prompt)\n",
    "\n",
    "jupyter_heatmap(tokens, lrp_relevances, cmap='bwr')"
   ],
   "id": "f582e3a8651fb1af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def attribute_prediction(model, tokenizer, prompt, top_k=3):\n",
    "    \"\"\"\n",
    "    Performs the forward pass and gradient calculation for top k tokens.\n",
    "    Now accepts model and tokenizer explicitly.\n",
    "    \"\"\"\n",
    "    top_k = int(top_k)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenization\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(model.device)\n",
    "    embeddings = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "    # Enable gradients on embeddings\n",
    "    embeddings.requires_grad_(True)\n",
    "\n",
    "    # Forward Pass\n",
    "    output = model(inputs_embeds=embeddings, use_cache=False)\n",
    "    output_logits = output.logits\n",
    "\n",
    "    # Select top k at the last token position\n",
    "    max_logits, max_indices = torch.topk(output_logits[0, -1, :], dim=-1, k=top_k)\n",
    "\n",
    "    results = {}\n",
    "    tokens_text = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Assuming clean_tokens is defined elsewhere in your utils\n",
    "    cleaned_tokens = clean_tokens(tokens_text)\n",
    "\n",
    "    for i in range(top_k):\n",
    "        target_token_id = max_indices[i]\n",
    "        target_token_str = tokenizer.decode(target_token_id)\n",
    "        score = max_logits[i]\n",
    "\n",
    "        model.zero_grad()\n",
    "        if embeddings.grad is not None:\n",
    "            embeddings.grad.zero_()\n",
    "\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        r = (embeddings.grad * embeddings).float().sum(-1).squeeze().detach().cpu()\n",
    "        r = r / r.abs().max()\n",
    "        heatmap_data = list(zip(cleaned_tokens, r.numpy()))\n",
    "\n",
    "        results[f\"{target_token_str} (Logit: {score.item():.2f})\"] = heatmap_data\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_analysis(model, tokenizer, prompt, top_k):\n",
    "    \"\"\"Triggered when user clicks 'Analyze'\"\"\"\n",
    "    if not prompt:\n",
    "        return None, gr.Radio(choices=[]), []\n",
    "\n",
    "    print(f\"Analyzing '{prompt}' with Top-K={top_k}\")\n",
    "\n",
    "    # Pass model and tokenizer down to the interpretation logic\n",
    "    results = attribute_prediction(model, tokenizer, prompt, top_k=top_k)\n",
    "    keys = list(results.keys())\n",
    "\n",
    "    return results, gr.Radio(choices=keys, value=keys[0], interactive=True), results[keys[0]]\n",
    "\n",
    "def update_heatmap(selected_token, results):\n",
    "    if selected_token in results:\n",
    "        return results[selected_token]\n",
    "    return []\n",
    "\n",
    "def launch_relevance_visualizer(model, tokenizer, default_prompt=\"The capital of France is \", port=7873):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: The loaded HuggingFace model.\n",
    "        tokenizer: The loaded HuggingFace tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    with gr.Blocks(title=\"LLM Relevance Heatmap\") as demo:\n",
    "        gr.Markdown(\"## LLM Token Relevance Visualizer with LRP\")\n",
    "        gr.Markdown(\"Enter a prompt to see top predicted next tokens and input relevance.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                prompt_input = gr.Textbox(label=\"Input Prompt\", value=default_prompt, lines=2)\n",
    "                top_k_slider = gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Top K Predictions\")\n",
    "                analyze_btn = gr.Button(\"Calculate Relevance\", variant=\"primary\")\n",
    "                prediction_selector = gr.Radio(choices=[], label=\"Predicted Next Token\", interactive=True)\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                heatmap_display = gr.HighlightedText(\n",
    "                    label=\"Relevance Heatmap\",\n",
    "                    combine_adjacent=False,\n",
    "                    show_legend=True,\n",
    "                    color_map={\"High Relevance\": \"#ff4b4b\", \"Medium Relevance\": \"#ff9f4b\", \"Low Relevance\": \"#ffe54b\"}\n",
    "                )\n",
    "\n",
    "        analysis_state = gr.State()\n",
    "\n",
    "        run_analysis_with_model = functools.partial(run_analysis, model, tokenizer)\n",
    "\n",
    "        analyze_btn.click(\n",
    "            fn=run_analysis_with_model,\n",
    "            inputs=[prompt_input, top_k_slider],\n",
    "            outputs=[analysis_state, prediction_selector, heatmap_display]\n",
    "        )\n",
    "\n",
    "        prediction_selector.change(\n",
    "            fn=update_heatmap,\n",
    "            inputs=[prediction_selector, analysis_state],\n",
    "            outputs=[heatmap_display]\n",
    "        )\n",
    "\n",
    "    demo.launch(inline=True, server_port=port)\n",
    "    return demo"
   ],
   "id": "e67e404ebe498ee1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The relevance scores are conditioned on the target. Changing the explained token alters the distribution of relevance across the input.",
   "id": "accd468fec3c4a8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Launch Gradio\n",
    "demo = launch_relevance_visualizer(model, tokenizer, default_prompt=prompt, port=1313)"
   ],
   "id": "4d65743d073cfee0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main Act: The In-Context Learning Brain",
   "id": "39f7ecc771dd586"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This widget allows you to manually inspect the raw attention weights for any specific layer and head.\n",
   "id": "116b8207993f6924"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "attentions = outputs.attentions"
   ],
   "id": "90f966280acee6bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "attentions[0].shape",
   "id": "abb99bc83e6dc5c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot the attention maps\n",
    "def visualize_llama_attention(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    def plot_attention(layer_idx, head_idx):\n",
    "        # Retrieve specific attention matrix\n",
    "        # Layer shape: [batch, heads, seq_len, seq_len] -> select [0, head_idx, :, :]\n",
    "        attn_map = attentions[layer_idx][0, head_idx].float().cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            attn_map,\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap=\"viridis\",\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Attention Weight'}\n",
    "        )\n",
    "        plt.title(f\"Layer {layer_idx} | Head {head_idx}\")\n",
    "        plt.xlabel(\"Key Tokens (Attended To)\")\n",
    "        plt.ylabel(\"Query Tokens (Attending From)\")\n",
    "        plt.show()\n",
    "\n",
    "    num_layers = len(attentions)\n",
    "    num_heads = attentions[0].shape[1]\n",
    "\n",
    "    layer_slider = widgets.IntSlider(value=0, min=0, max=num_layers-1, description='Layer:')\n",
    "    head_slider = widgets.IntSlider(value=0, min=0, max=num_heads-1, description='Head:')\n",
    "\n",
    "    ui = widgets.HBox([layer_slider, head_slider])\n",
    "    out = widgets.interactive_output(plot_attention, {'layer_idx': layer_slider, 'head_idx': head_slider})\n",
    "\n",
    "    display(ui, out)\n",
    "\n",
    "# Run\n",
    "visualize_llama_attention(prompt)"
   ],
   "id": "64d92b1cde8791ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß™ Experiment 4: Attention Head Relevance",
   "id": "d3a2468fc93f9f30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now switch from input relevance to component relevance. Instead of only looking at the gradients when they reach the inputs, we intercept them as they pass through the attention layers. By retaining the gradients at this intermediate stage, we can calculate Activation √ó Gradient for every specific head. This tells us not just what the model looked at, but which internal component was responsible for the predicted token.",
   "id": "d475a7b1911c7bc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def attribute_attn_heads(model, input_ids, target=None):\n",
    "    \"\"\"\n",
    "    Computes Input * Gradient attribution for the last token position.\n",
    "    Uses gradients from softmax outputs (attention weights).\n",
    "    \"\"\"\n",
    "    # Get embeddings and enable gradient tracking\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    inputs_embeds = embedding_layer(input_ids)\n",
    "    inputs_embeds.requires_grad = True\n",
    "\n",
    "    # Forward pass with attention outputs\n",
    "    outputs = model(inputs_embeds=inputs_embeds, use_cache=False, output_attentions=True)\n",
    "\n",
    "    # Retain gradients for attention weights (softmax outputs)\n",
    "    for attn_layer in outputs.attentions:\n",
    "        attn_layer.retain_grad()\n",
    "\n",
    "    # If target is not given, use the predicted token\n",
    "    if target is None:\n",
    "        # Get the predicted token at last position (argmax of last position)\n",
    "        target = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "\n",
    "    target_score = outputs.logits[0, -1, target]\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    target_score.backward()\n",
    "\n",
    "    # Get attention gradients (if needed for additional analysis)\n",
    "    attn_relevance = []\n",
    "    attn_activation = []\n",
    "    for attn_layer in outputs.attentions:\n",
    "        activation = attn_layer.float().detach().cpu()\n",
    "        attn_activation.append(activation)\n",
    "\n",
    "        relevance = attn_layer * attn_layer.grad\n",
    "        attn_relevance.append(relevance.float().detach().cpu())\n",
    "\n",
    "    return torch.cat(attn_activation), torch.cat(attn_relevance), target"
   ],
   "id": "ba9fca18c8c9248c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get input ids\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# Calculate relevances per head and token\n",
    "_, attn_relevance, _ = attribute_attn_heads(model, input_ids)\n",
    "\n",
    "# Sum up relevances per head\n",
    "attn_relevance = torch.sum(attn_relevance, dim=(-2, -1)).flatten() # shape: [num_heads] [512]\n",
    "\n",
    "# Sort values in descending order (highest relevance first)\n",
    "sorted_values = torch.sort(attn_relevance, descending=True).values.cpu().numpy()\n",
    "\n",
    "# Plot the sorted values as a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Use bar chart to show each head as a distinct entity\n",
    "plt.bar(range(len(sorted_values)), sorted_values, color='darkblue', width=0.6, align='edge')\n",
    "plt.title('Relevance Scores of All Heads (Sorted)')\n",
    "plt.xlabel('Heads (Ordered by Relevance)')\n",
    "plt.ylabel('Relevance Score')\n",
    "\n",
    "# Set x-axis limit to match the number of heads\n",
    "plt.xlim(0, len(sorted_values))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ],
   "id": "6245ecc71f8e57d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üí° 1. Takeaway: Sparsity\n",
    "\n",
    "We calculated the aggregate relevance of every attention head. The bar chart reveals that the vast majority of heads are not relevant, with only a few doing the heavy lifting."
   ],
   "id": "abbcbf8230005f43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clustering of Attention Weight Relevances\n",
    "\n",
    "This function projects the attention heads into a 2D space (using UMAP) and clusters them (using KMeans) based on their relevance patterns."
   ],
   "id": "114cd95d0a9eac1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cluster the attention maps\n",
    "\n",
    "def attention_relevance_explorer(\n",
    "    text,\n",
    "    top_k=100,\n",
    "    n_clusters=5,\n",
    "    color_by='cluster'\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactive attention relevance explorer using jscatter.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        top_k: Number of top heads to visualize\n",
    "        n_clusters: Number of clusters for KMeans\n",
    "        color_by: 'cluster' or 'layer'\n",
    "    \"\"\"\n",
    "    print(f\"--- Attention Relevance Explorer ---\")\n",
    "    print(f\"1. Analyzing top {top_k} heads by relevance magnitude\")\n",
    "    print(f\"2. Clustering into {n_clusters} groups\")\n",
    "\n",
    "    # Tokenize and encode\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n",
    "\n",
    "    # Get relevance attribution\n",
    "    print(\"3. Computing relevance (Input * Gradient)...\")\n",
    "    attn_activation, attn_relevance, target = attribute_attn_heads(model, encoded.input_ids.to(device))\n",
    "\n",
    "    # Shape: [layers, heads, seq, seq] -> [layers * heads, seq, seq]\n",
    "    num_layers = attn_relevance.shape[0]\n",
    "    num_heads = attn_relevance.shape[1]\n",
    "    attn_relevance = attn_relevance.reshape(num_layers * num_heads, attn_relevance.shape[2], attn_relevance.shape[3])\n",
    "\n",
    "    print(f\"   Predicted token: '{tokenizer.decode(target)}'\")\n",
    "\n",
    "    # Compute head-level scores (sum over all query-key positions)\n",
    "    # Shape: [layers * heads, seq, seq] -> [layers * heads]\n",
    "    head_relevance = torch.sum(attn_relevance, dim=(-1, -2))\n",
    "\n",
    "    # Get top heads\n",
    "    top_indices = torch.topk(head_relevance.flatten(), top_k).indices\n",
    "\n",
    "    # Extract data for visualization\n",
    "    visualization_data = []\n",
    "\n",
    "    for idx in top_indices:\n",
    "        idx = idx.item()\n",
    "        layer = idx // num_heads\n",
    "        head = idx % num_heads\n",
    "\n",
    "        # Get full attention relevance map\n",
    "        # Shape: [seq, seq] (queries x keys)\n",
    "        attn_map = attn_relevance[idx].numpy()\n",
    "        score = head_relevance[idx].item()\n",
    "\n",
    "        visualization_data.append({\n",
    "            'layer': layer,\n",
    "            'head': head,\n",
    "            'label': f\"L{layer}-H{head}\",\n",
    "            'score': score,\n",
    "            'attn_map': attn_map,\n",
    "            'vector': attn_map.flatten()  # Flattened for UMAP\n",
    "        })\n",
    "\n",
    "    # Prepare data for UMAP (flatten 2D maps)\n",
    "    X = np.array([d['vector'] for d in visualization_data])\n",
    "\n",
    "    if len(X) < 2:\n",
    "        print(\"(!) Error: Not enough heads for visualization.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # UMAP dimensionality reduction\n",
    "    print(\"4. Running UMAP...\")\n",
    "    n_neighbors = min(15, len(X) - 1)\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    embedding = reducer.fit_transform(X)\n",
    "\n",
    "    # Create dataframe (exclude attn_map from df)\n",
    "    df = pd.DataFrame([{k: v for k, v in d.items() if k not in ['attn_map', 'vector']} for d in visualization_data])\n",
    "    df['x'] = embedding[:, 0]\n",
    "    df['y'] = embedding[:, 1]\n",
    "\n",
    "    # KMeans clustering\n",
    "    if n_clusters > 1:\n",
    "        print(f\"5. Clustering with KMeans (k={n_clusters})...\")\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        df['cluster'] = kmeans.fit_predict(embedding)\n",
    "    else:\n",
    "        df['cluster'] = 0\n",
    "\n",
    "    # Determine coloring\n",
    "    if color_by == 'cluster' and n_clusters > 1:\n",
    "        color_col = 'cluster'\n",
    "        cmap = 'tab10'\n",
    "    else:\n",
    "        color_col = 'layer'\n",
    "        cmap = 'viridis'\n",
    "\n",
    "    # Create metadata dataframe for scatter\n",
    "    df_meta = df.copy()\n",
    "\n",
    "    # Interactive visualization\n",
    "    print(\"6. Creating interactive plot...\")\n",
    "\n",
    "    detail_view = widgets.Output(\n",
    "        layout={'border': '1px solid gray', 'width': '50%', 'height': '600px', 'overflow': 'auto'}\n",
    "    )\n",
    "\n",
    "    def update_detail_view(change):\n",
    "        selection = change['new']\n",
    "        detail_view.clear_output(wait=True)\n",
    "\n",
    "        with detail_view:\n",
    "            if not selection:\n",
    "                print(\"Click a point to see the full attention relevance map.\\n\")\n",
    "                print(\"Y-axis: Query positions (which token is attending)\")\n",
    "                print(\"X-axis: Key positions (which token is attended to)\")\n",
    "                return\n",
    "\n",
    "            idx = selection[0]\n",
    "            data = visualization_data[idx]\n",
    "            attn_map = data['attn_map']\n",
    "\n",
    "            cluster_label = f\" | Cluster {df.iloc[idx]['cluster']}\" if n_clusters > 1 else \"\"\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "            # Plot heatmap with diverging colormap (red-blue)\n",
    "            attn_map = attn_map / abs(attn_map).max()\n",
    "            im = ax.imshow(attn_map, cmap='RdBu_r', aspect='auto', interpolation='nearest', vmin=-1, vmax=1)\n",
    "\n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax, label='Relevance (Activation √ó Gradient)')\n",
    "\n",
    "            # Set title\n",
    "            ax.set_title(\n",
    "                f\"{data['label']}{cluster_label}\\nTotal Relevance Score: {data['score']:.4f}\",\n",
    "                fontsize=12,\n",
    "                pad=20\n",
    "            )\n",
    "\n",
    "            # Set axis labels\n",
    "            ax.set_ylabel(\"Query Position (Token)\", fontsize=10)\n",
    "            ax.set_xlabel(\"Key Position (Token)\", fontsize=10)\n",
    "\n",
    "            # Set ticks and labels\n",
    "            seq_len = len(tokens)\n",
    "            tick_positions = list(range(seq_len))\n",
    "            ax.set_yticks(tick_positions)\n",
    "            ax.set_yticklabels(tokens, fontsize=8)\n",
    "            ax.set_xticks(tick_positions)\n",
    "            ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "            # Add grid for better readability\n",
    "            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Create scatter plot\n",
    "    scatter = Scatter(\n",
    "        data=df_meta,\n",
    "        x='x',\n",
    "        y='y',\n",
    "        color_by=color_col,\n",
    "        colormap=cmap,\n",
    "        size=10,\n",
    "        height=500,\n",
    "        opacity=0.9,\n",
    "        tooltip=True,\n",
    "        legend=True,\n",
    "        background_color='white',\n",
    "        axes=True\n",
    "    )\n",
    "\n",
    "    scatter.widget.observe(update_detail_view, names='selection')\n",
    "\n",
    "    # Display\n",
    "    display(HTML(\"\"\"\n",
    "        <style>\n",
    "        .white-bg-fix {\n",
    "            background-color: white !important;\n",
    "            color: black !important;\n",
    "        }\n",
    "        </style>\n",
    "    \"\"\"))\n",
    "\n",
    "    container = widgets.HBox([scatter.show(), detail_view])\n",
    "    container.add_class(\"white-bg-fix\")\n",
    "    display(container)\n",
    "\n",
    "    print(\"\\n‚úì Done! Click on points to explore full attention head relevance maps.\")\n",
    "    print(\"   Heatmap: Y-axis = Queries, X-axis = Keys\")\n",
    "\n",
    "    return df_meta[['layer', 'head', 'label', 'score', 'cluster', 'x', 'y']]"
   ],
   "id": "db4e841585b4a8d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run\n",
    "df_results = attention_relevance_explorer(\n",
    "    text=prompt,\n",
    "    top_k=150,\n",
    "    n_clusters=20,\n",
    "    color_by='cluster'\n",
    ")"
   ],
   "id": "c93fe8d27f9a5421"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üí° 2. Takeaway: Functional Specialization\n",
    "The clustering shows that attention heads are specialized. Just as the brain has distinct regions for vision and language, the Transformer has distinct clusters of heads for different tasks.\n"
   ],
   "id": "12358b29f64d5477"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Function Vectors: The Causal Test\n",
   "id": "d9d96c0b82bdefc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß™ Experiment 5: Task Head Transplantation\n",
   "id": "a5eb681822d2bf2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We define hooks to intervene in the model's forward pass. This allows us to transplant the output of specific attention heads from one context to another.",
   "id": "5ed611e327980173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_hook(head_ids, function_vectors, model_config, scaling):\n",
    "    \"\"\"Creates a hook to manipulate attention head outputs.\n",
    "    \n",
    "    Args:\n",
    "        head_ids: List of head indices within this layer\n",
    "        function_vectors: Tensor of shape [batch, seq_len, num_heads, head_dim]\n",
    "        model_config: Model configuration\n",
    "        scaling: Scaling factor for function vectors\n",
    "    \"\"\"\n",
    "    def manipulate_input(module, input, output):\n",
    "        # input[0] is attention output: [batch, seq_len, hidden_size]\n",
    "        bsz, q_len, hidden_size = input[0].shape\n",
    "        num_heads = model_config.num_attention_heads\n",
    "        head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Reshape to separate heads: [batch, seq_len, num_heads, head_dim]\n",
    "        attn_output = input[0].view(bsz, q_len, num_heads, head_dim)\n",
    "        \n",
    "        # Replace specified heads at last position with scaled function vectors\n",
    "        attn_output[:, -1, head_ids] = function_vectors[:, -1] * scaling\n",
    "        \n",
    "        # Reshape back and apply output projection\n",
    "        attn_output = attn_output.reshape(bsz, q_len, hidden_size)\n",
    "        return torch.nn.functional.linear(attn_output, module.weight, module.bias)\n",
    "    \n",
    "    return manipulate_input\n",
    "\n",
    "\n",
    "def patch_function_vectors_(model, inputs, head_ids, scaling=2):\n",
    "    \"\"\"Patches function vectors from head_ids into the model.\n",
    "    \n",
    "    Args:\n",
    "        model: LlamaForCausalLM model\n",
    "        inputs: Input token IDs from seed prompt\n",
    "        head_ids: List of global head IDs to patch\n",
    "        scaling: Scaling factor for function vectors\n",
    "        \n",
    "    Returns:\n",
    "        handles: List of hook handles for cleanup\n",
    "    \"\"\"\n",
    "    num_heads_per_layer = model.config.num_attention_heads\n",
    "    head_dim = model.config.hidden_size // num_heads_per_layer\n",
    "    \n",
    "    # Group heads by layer\n",
    "    head_dict = {}\n",
    "    for head_id in head_ids:\n",
    "        layer_id = head_id // num_heads_per_layer\n",
    "        head_dict.setdefault(layer_id, []).append(head_id)\n",
    "    \n",
    "    # Capture attention outputs from relevant layers\n",
    "    o_proj_inputs_cache = {}\n",
    "    \n",
    "    def capture_o_proj_input(layer_id):\n",
    "        def hook(module, input, output):\n",
    "            o_proj_inputs_cache[layer_id] = input[0].detach().clone()\n",
    "        return hook\n",
    "    \n",
    "    # Register capture hooks\n",
    "    capture_handles = [\n",
    "        model.model.layers[layer_id].self_attn.o_proj.register_forward_hook(\n",
    "            capture_o_proj_input(layer_id)\n",
    "        )\n",
    "        for layer_id in head_dict.keys()\n",
    "    ]\n",
    "    \n",
    "    # Run forward pass to capture activations\n",
    "    model(input_ids=inputs, use_cache=False)\n",
    "    \n",
    "    # Remove capture hooks\n",
    "    for handle in capture_handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    # Register manipulation hooks\n",
    "    handles = []\n",
    "    for layer_id, global_head_ids in head_dict.items():\n",
    "        # Extract function vectors for this layer's heads\n",
    "        o_proj_input = o_proj_inputs_cache[layer_id]\n",
    "        \n",
    "        # Convert global head IDs to layer-local indices\n",
    "        local_head_ids = [h_id % num_heads_per_layer for h_id in global_head_ids]\n",
    "        \n",
    "        # Extract and stack function vectors: [batch, seq_len, num_heads, head_dim]\n",
    "        function_vectors = torch.stack([\n",
    "            o_proj_input[:, :, h_idx * head_dim:(h_idx + 1) * head_dim]\n",
    "            for h_idx in local_head_ids\n",
    "        ], dim=2)\n",
    "        \n",
    "        # Register manipulation hook\n",
    "        handle = model.model.layers[layer_id].self_attn.o_proj.register_forward_hook(\n",
    "            create_hook(local_head_ids, function_vectors, model.config, scaling)\n",
    "        )\n",
    "        handles.append(handle)\n",
    "    \n",
    "    return handles"
   ],
   "id": "89686ac982de3daa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We patch the vectors from our identified clusters into a new prompt (about the Atlantic Ocean) to see if we can trigger specific behaviors causally.",
   "id": "2bfbeb8544db04e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Setup\n",
    "unique_clusters = sorted(df_results['cluster'].unique())\n",
    "print(f\"Found {len(unique_clusters)} clusters: {unique_clusters}\")\n",
    "\n",
    "num_heads = model.config.num_attention_heads\n",
    "target_prompt = \"The Atlantic Ocean is the second-largest ocean on Earth. It holds about 310 million cubic kilometers of water.\" \\\n",
    "\"It spans a wide range of marine biomes, from warm tropical coral reef systems near the equator to cold, nutrient-rich polar waters in the north and south. \" \\\n",
    "\"These biomes support diverse life, including plankton, fish, whales, and deep-sea organisms, and play a major role in regulating Earth's climate through ocean currents like the Gulf Stream.\" \\\n",
    "\"Q: Which biomes does it support?\\nA: \"\n",
    "\n",
    "\n",
    "# 2. Prepare Inputs\n",
    "# A. Source Input\n",
    "inputs_source = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# B. Target Input\n",
    "inputs_target = tokenizer(target_prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "\n",
    "# 3. Normal Generation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BASELINE: Target Generation (No Patching)\")\n",
    "print(\"=\" * 60)\n",
    "generated_ids = model.generate(input_ids=inputs_target.input_ids, do_sample=False, max_new_tokens=50)\n",
    "decoded_normal = tokenizer.decode(generated_ids[0, inputs_target.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(decoded_normal.strip())\n",
    "print(\"\\n\" + \"#\" * 80 + \"\\nSTARTING CLUSTER INTERVENTIONS\\n\" + \"#\" * 80 + \"\\n\")\n",
    "\n",
    "# 4. Cluster Loop\n",
    "for cluster_id in unique_clusters:\n",
    "    print(f\"PROCESSING CLUSTER: {cluster_id}\")\n",
    "\n",
    "    # A. Filter for Current Cluster\n",
    "    cluster_data = df_results[df_results['cluster'] == cluster_id]\n",
    "\n",
    "    # B. Calculate Global IDs\n",
    "    head_ids_to_patch = (cluster_data['layer'] * num_heads + cluster_data['head']).tolist()\n",
    "\n",
    "    # Configuration for this run\n",
    "    scaling = 2.0\n",
    "\n",
    "    print(f\" -> Patching {len(head_ids_to_patch)} heads: {head_ids_to_patch}\")\n",
    "\n",
    "    # C. Patched Generation\n",
    "    handles = patch_function_vectors_(model, inputs_source.input_ids, head_ids_to_patch, scaling=scaling)\n",
    "\n",
    "    generated_ids = model.generate(input_ids=inputs_target.input_ids, do_sample=False, max_new_tokens=50)\n",
    "    print(generated_ids.shape)\n",
    "    decoded_patched = tokenizer.decode(generated_ids[0, inputs_target.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"PATCHED OUTPUT (Cluster {cluster_id})\")\n",
    "    print(\"-\" * 40)\n",
    "    print(decoded_patched.strip())\n",
    "\n",
    "    # D. Remove hooks\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    print(f\"[Hooks removed]\\n\")"
   ],
   "id": "6907cd8314723ef4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This function identifies which attention heads are most relevant with respect to a specific substring.",
   "id": "7ed9372145a0173c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "attentions[0].shape",
   "id": "f77c58942da5d4fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_topk_heads_on_substring(text, search_string, top_k=20, verbose=False):\n",
    "    \"\"\"\n",
    "    Analyzes attention heads focusing on a substring.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (head_ids, df)\n",
    "            - head_ids (list): List of global head IDs [int]\n",
    "            - df (pd.DataFrame): Data with columns ['layer', 'head', 'score', 'type']\n",
    "    \"\"\"\n",
    "    # 1. Tokenize\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n",
    "\n",
    "    # 2. Find substring tokens\n",
    "    search_encoded = tokenizer(search_string, add_special_tokens=False)\n",
    "    search_tokens = tokenizer.convert_ids_to_tokens(search_encoded.input_ids)\n",
    "\n",
    "    # Find matching token indices\n",
    "    key_indices = []\n",
    "    for i in range(len(tokens) - len(search_tokens) + 1):\n",
    "        match = True\n",
    "        for j, search_tok in enumerate(search_tokens):\n",
    "            token_clean = tokens[i + j].replace('‚ñÅ', '').replace('ƒ†', '').lower()\n",
    "            search_clean = search_tok.replace('‚ñÅ', '').replace('ƒ†', '').lower()\n",
    "            if token_clean != search_clean:\n",
    "                match = False\n",
    "                break\n",
    "        if match:\n",
    "            key_indices.extend(range(i, i + len(search_tokens)))\n",
    "\n",
    "    key_indices = sorted(list(set(key_indices)))\n",
    "\n",
    "    # Handle missing substring\n",
    "    if not key_indices:\n",
    "        if verbose:\n",
    "            print(f\"Warning: Could not find tokens for '{search_string}'\")\n",
    "        return [], pd.DataFrame(columns=['layer', 'head', 'score', 'type'])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Found substring at positions {key_indices}: {[tokens[i] for i in key_indices]}\")\n",
    "\n",
    "    # 3. Get relevance\n",
    "    _, attn_relevance, target = attribute_attn_heads(model, encoded.input_ids.to(device))\n",
    "\n",
    "    # Reshape to [layers * heads, seq, seq]\n",
    "    num_layers = attn_relevance.shape[0]\n",
    "    num_heads = attn_relevance.shape[1]\n",
    "    attn_relevance = attn_relevance.reshape(num_layers * num_heads, attn_relevance.shape[2], attn_relevance.shape[3])\n",
    "\n",
    "    # 4. Calculate Scores\n",
    "    last_query_relevance = attn_relevance[:, -1, :]\n",
    "    substring_relevance = last_query_relevance[:, key_indices].clamp(min=0).sum(dim=-1)\n",
    "\n",
    "    # 5. Extract Top-K\n",
    "    top_results = torch.topk(substring_relevance, top_k)\n",
    "    top_indices = top_results.indices\n",
    "    top_scores = top_results.values\n",
    "\n",
    "    # 6. Build Outputs\n",
    "    head_ids = top_indices.tolist()\n",
    "    results_data = []\n",
    "\n",
    "    for idx_tensor, score_tensor in zip(top_indices, top_scores):\n",
    "        global_idx = idx_tensor.item()\n",
    "        score = score_tensor.item()\n",
    "\n",
    "        layer = global_idx // num_heads\n",
    "        head = global_idx % num_heads\n",
    "\n",
    "        results_data.append({\n",
    "            'layer': int(layer),\n",
    "            'head': int(head),\n",
    "            'score': score,\n",
    "            'type': search_string\n",
    "        })\n",
    "\n",
    "        if verbose and len(results_data) <= 10:\n",
    "             print(f\"  L{layer}-H{head} (ID={global_idx}): {score:.4f}\")\n",
    "\n",
    "    df = pd.DataFrame(results_data)\n",
    "\n",
    "    return head_ids, df"
   ],
   "id": "7d698ab3e538a9b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We identify heads responsible for the task and patch them into the target prompt to see if we can force the model to answer a different question.",
   "id": "e793492d57e253e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get heads focusing on the Task\n",
    "\n",
    "input_ids_seed = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(device)\n",
    "\n",
    "HEAD_IDS, df_task = get_topk_heads_on_substring(\n",
    "    text=prompt,\n",
    "    search_string=\"Q: How much dust is blown from the Sahara each year\",\n",
    "    top_k=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nHead IDs: {HEAD_IDS}\")"
   ],
   "id": "bec373836941281f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Patch these heads into the Atlantic Ocean prompt\n",
    "input_ids_target = tokenizer(target_prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "\n",
    "# Normal generation\n",
    "generated_ids = model.generate(input_ids=input_ids_target.input_ids, do_sample=False, max_new_tokens=10)\n",
    "print(\"Normal:\", tokenizer.decode(generated_ids[0, input_ids_target.input_ids.shape[1]:]))\n",
    "\n",
    "# Patched generation\n",
    "try:\n",
    "    handles = patch_function_vectors_(model, input_ids_seed, HEAD_IDS, scaling=2.0)\n",
    "    generated_ids = model.generate(input_ids=input_ids_target.input_ids, do_sample=False, max_new_tokens=50)\n",
    "    print(\"Patched:\", tokenizer.decode(generated_ids[0, input_ids_target.input_ids.shape[1]:]))\n",
    "finally:\n",
    "    [h.remove() for h in handles]"
   ],
   "id": "f602bf51b0348fae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also identify heads used for translation and patch them into a non-translation prompt.",
   "id": "818317ebedf9da16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Translation\n",
    "seed_prompt_translation   = \"\"\"\\\n",
    "Task: Translation into German: I love AI research\n",
    "Answer:\"\"\"\n",
    "\n",
    "input_ids_seed = tokenizer(seed_prompt_translation, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(device)\n",
    "\n",
    "HEAD_IDS, df_translation = get_topk_heads_on_substring(\n",
    "    text=seed_prompt_translation,\n",
    "    search_string=\"Translation into German\",\n",
    "    top_k=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nHead IDs: {HEAD_IDS}\")"
   ],
   "id": "db446ab365d30491"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Patch these heads into a new prompt\n",
    "target_prompt_translation = \"Write the opposite of this sentence: The car is going very fast.\\nAnswer: \"\n",
    "inputs_test = tokenizer(target_prompt_translation, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "\n",
    "# Normal generation\n",
    "generated_ids = model.generate(input_ids=inputs_test.input_ids, do_sample=False, max_new_tokens=20)\n",
    "print(\"Normal:\", tokenizer.decode(generated_ids[0, inputs_test.input_ids.shape[1]:]))\n",
    "\n",
    "# Patched generation\n",
    "try:\n",
    "    handles = patch_function_vectors_(model, input_ids_seed, HEAD_IDS, scaling=3.0)\n",
    "    generated_ids = model.generate(input_ids=inputs_test.input_ids, do_sample=False, max_new_tokens=50)\n",
    "    print(\"Patched:\", tokenizer.decode(generated_ids[0, inputs_test.input_ids.shape[1]:]))\n",
    "finally:\n",
    "    [handle.remove() for handle in handles]"
   ],
   "id": "9274472cc1147be5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß™ Experiment 6: Retrieval Head Manipulation\n",
   "id": "f36cfa7b0ab6255"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We isolate the specific heads responsible for retrieving the answer \"182 million tons.\"",
   "id": "f8afd7ce77015223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get heads focusing on \"182 million tons\"\n",
    "input_ids_seed = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(device)\n",
    "\n",
    "HEAD_IDS, df_retrival = get_topk_heads_on_substring(\n",
    "    text=prompt,\n",
    "    search_string=\"182 million tons\",\n",
    "    top_k=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nHead IDs: {HEAD_IDS}\")"
   ],
   "id": "7de37530bafcec4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Patch these heads into the Atlantic Ocean prompt\n",
    "inputs_test = tokenizer(target_prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "\n",
    "# Normal generation\n",
    "generated_ids = model.generate(input_ids=inputs_test.input_ids, do_sample=False, max_new_tokens=10)\n",
    "print(\"Normal:\", tokenizer.decode(generated_ids[0, inputs_test.input_ids.shape[1]:]))\n",
    "\n",
    "# Patched generation\n",
    "try:\n",
    "    handles = patch_function_vectors_(model, input_ids_seed, HEAD_IDS, scaling=1.0)\n",
    "    generated_ids = model.generate(input_ids=inputs_test.input_ids, do_sample=False, max_new_tokens=50)\n",
    "    print(\"Patched:\", tokenizer.decode(generated_ids[0, inputs_test.input_ids.shape[1]:]))\n",
    "finally:\n",
    "    [h.remove() for h in handles]"
   ],
   "id": "599f2d28e708a62e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see where these heads are located throughout the model layers",
   "id": "c473e1048a3b33b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot Atlas for task heads and retrival heads\n",
    "\n",
    "# Combine dfs\n",
    "df_combined = pd.concat([df_task, df_translation, df_retrival], ignore_index=True)\n",
    "\n",
    "# Plot\n",
    "plot_atlas(df_combined, color_col='type', size_col='score',top_k=10)"
   ],
   "id": "d382519a6163a42a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üí° 3. Takeaway: Distributed Representation\n",
    "\n",
    "When we map these heads, a clear depth-wise organization emerges. Task Heads are primarily found in the middle layers, acting as the processing engine. In contrast, Retrieval Heads are concentrated in the later layers, refining the information just before the final output\n"
   ],
   "id": "856e622f7470ea67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The Parametric Knowledge Brain: A Different World\n",
   "id": "3fdc7892dd27710a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß™ Experiment 7: Knowledge Conflict\n",
   "id": "caed650f0598c30c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We define a dataset of \"Counterfactual\" examples (e.g., \"The capital of France is London\") where the provided context contradicts the model's internal (parametric) knowledge.",
   "id": "8dd358a7d50d713e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SAMPLES = [\n",
    "    # Geography\n",
    "    {'param_answer': 'Paris', 'context_answer': 'London',\n",
    "     'prompt': \"The capital of France is London.\\nQ: What is the capital of France? A: The capital of France is\"},\n",
    "    {'param_answer': 'Berlin', 'context_answer': 'Paris',\n",
    "     'prompt': \"The capital of Germany is Paris.\\nQ: What is the capital of Germany? A: The capital of Germany is\"},\n",
    "    {'param_answer': 'Rome', 'context_answer': 'Milan',\n",
    "     'prompt': \"The capital of Italy is Milan.\\nQ: What is the capital of Italy? A: The capital of Italy is\"},\n",
    "\n",
    "    # Brands\n",
    "    {'param_answer': 'Apple', 'context_answer': 'Samsung',\n",
    "     'prompt': \"The iPhone is made by Samsung.\\nQ: Who makes the iPhone? A: The iPhone is made by\"},\n",
    "    {'param_answer': 'Tesla', 'context_answer': 'Ford',\n",
    "     'prompt': \"Elon Musk runs Ford.\\nQ: What company does Elon Musk run? A: Elon Musk runs\"},\n",
    "    {'param_answer': 'Google', 'context_answer': 'Yahoo',\n",
    "     'prompt': \"Yahoo owns YouTube.\\nQ: Who owns YouTube? A: YouTube is owned by\"},\n",
    "\n",
    "    # Colors\n",
    "    {'param_answer': 'white', 'context_answer': 'green',\n",
    "     'prompt': \"The color of snow is green.\\nQ: What is the color of snow? A: The color of snow is\"},\n",
    "    {'param_answer': 'blue', 'context_answer': 'red',\n",
    "     'prompt': \"The color of the sky is red.\\nQ: What is the color of the sky? A: The color of the sky is\"},\n",
    "    {'param_answer': 'yellow', 'context_answer': 'purple',\n",
    "     'prompt': \"The color of the sun is purple.\\nQ: What is the color of the sun? A: The color of the sun is\"},\n",
    "\n",
    "    # Math\n",
    "    {'param_answer': '4', 'context_answer': '5',\n",
    "     'prompt': \"2 + 2 = 5.\\nQ: What is 2 + 2? A: 2 + 2 =\"},\n",
    "    {'param_answer': '6', 'context_answer': '7',\n",
    "     'prompt': \"3 + 3 = 7.\\nQ: What is 3 + 3? A: 3 + 3 =\"},\n",
    "    {'param_answer': '10', 'context_answer': '11',\n",
    "     'prompt': \"5 + 5 = 11.\\nQ: What is 5 + 5? A: 5 + 5 =\"},\n",
    "\n",
    "    # Animals\n",
    "    {'param_answer': 'dog', 'context_answer': 'cat',\n",
    "     'prompt': \"A cat barks.\\nQ: What animal barks? A: A\"},\n",
    "    {'param_answer': 'cat', 'context_answer': 'dog',\n",
    "     'prompt': \"A dog meows.\\nQ: What animal meows? A: A\"},\n",
    "    {'param_answer': 'cow', 'context_answer': 'goat',\n",
    "     'prompt': \"A goat gives milk on farms.\\nQ: What farm animal gives lots of milk? A: A\"},\n",
    "]"
   ],
   "id": "f3cff44a144fe5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_matching_token_in_top_k(word, top_tokens, tokenizer):\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    if len(tokens) == 0:\n",
    "        return None, None\n",
    "\n",
    "    first_token = tokens[0]\n",
    "    if first_token in top_tokens:\n",
    "        return first_token, top_tokens.index(first_token)\n",
    "\n",
    "    variations = [first_token, ' ' + word[:len(first_token)]]\n",
    "    for var in variations:\n",
    "        if var in top_tokens:\n",
    "            return var, top_tokens.index(var)\n",
    "\n",
    "    return None, None"
   ],
   "id": "a88cc1378bfd1c70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We run attribution on these samples to find which heads support the context answer vs. which heads support the parametric answer.",
   "id": "2bc86d4185382c7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process training samples (simplified, no intermediate `r` dict)\n",
    "TOP_HEADS = 25\n",
    "print(f\"\\nProcessing {len(SAMPLES)} training samples...\\n\")\n",
    "\n",
    "param_relevance, context_relevance = [],[]\n",
    "\n",
    "for i, sample in enumerate(SAMPLES):\n",
    "    input_ids = tokenizer(sample['prompt'], return_tensors='pt').input_ids.to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logits = model(input_ids).logits[0, -1]\n",
    "\n",
    "    top_ids = torch.topk(logits, 10).indices.tolist()\n",
    "    top_tokens = [tokenizer.decode([idx]).strip() for idx in top_ids]\n",
    "\n",
    "    param_match, param_idx = find_matching_token_in_top_k(sample['param_answer'], top_tokens, tokenizer)\n",
    "    context_match, context_idx = find_matching_token_in_top_k(sample['context_answer'], top_tokens, tokenizer)\n",
    "\n",
    "    if param_match is not None:\n",
    "        _, attn_relevance, _ = attribute_attn_heads(model, input_ids, target=top_ids[param_idx])\n",
    "        param_relevance.append(torch.sum(attn_relevance, dim=(-2, -1)))\n",
    "\n",
    "    if context_match is not None:\n",
    "        _, attn_relevance, _ = attribute_attn_heads(model, input_ids, target=top_ids[context_idx])\n",
    "        context_relevance.append(torch.sum(attn_relevance, dim=(-2, -1)))\n",
    "\n",
    "\n",
    "param_relevance = torch.stack(param_relevance)\n",
    "context_relevance = torch.stack(context_relevance)\n",
    "\n",
    "print(\"‚úì Done processing training samples.\")\n",
    "print(f\"Shape of param_relevance {param_relevance.shape} and context_relevance {context_relevance.shape}\\n\")"
   ],
   "id": "bb5b504dba40338a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use PCA to visualize the head relevances. This shows a clear separation between heads that focus on context and heads that focus on internal memory.",
   "id": "d5098ed946527f48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set style for better aesthetics\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "\n",
    "# Prepare data for PCA\n",
    "X_param = param_relevance.reshape(len(param_relevance), -1).cpu().numpy()\n",
    "X_ctx = context_relevance.reshape(len(context_relevance), -1).cpu().numpy()\n",
    "X = np.vstack([X_param, X_ctx])\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca.fit(X)\n",
    "pca_param = pca.transform(X_param)\n",
    "pca_ctx = pca.transform(X_ctx)\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# Plot PCA with enhanced styling\n",
    "scatter1 = ax.scatter(pca_param[:, 0], pca_param[:, 1], \n",
    "                      c='#06A77D', label='Param explained', \n",
    "                      s=80, alpha=0.6, edgecolors='white', linewidth=1.5)\n",
    "scatter2 = ax.scatter(pca_ctx[:, 0], pca_ctx[:, 1], \n",
    "                      c='#D62828', marker='*', s=400, \n",
    "                      label='Context explained', edgecolors='white', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax.set_title('PCA of Head-Level Relevances', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(frameon=True, shadow=True, fontsize=10, loc='best')\n",
    "ax.grid(True, alpha=0.3, linestyle=':', linewidth=1)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3cf5c8d322a0c74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Contrastive XAI\n",
    "\n",
    "We calculate a difference score (Parametric - Contextual) to identify the specific heads that are most strongly associated with each type of knowledge."
   ],
   "id": "d0b508c129680d40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GOAL: assigns a score to each attention head individually, indicating whether it is more relevant for parameter-based or context-based predictions.\n",
    "\n",
    "# Reminder: param_relevance.shape = (num_samples, num_layer, num_heads)\n",
    "#           context_relevance.shape = (num_samples, num_layer, num_heads)\n",
    "diff = param_relevance.mean(dim=0) - context_relevance.mean(dim=0)\n",
    "\n",
    "diff_flat = diff.flatten()\n",
    "num_heads = diff.shape[1]\n",
    "\n",
    "top_pos_idx = torch.topk(diff_flat, TOP_HEADS).indices\n",
    "top_neg_idx = torch.topk(diff_flat, TOP_HEADS, largest=False).indices\n",
    "selected_idx = torch.cat([top_pos_idx, top_neg_idx])\n",
    "\n",
    "selected_heads = [(idx.item() // num_heads, idx.item() % num_heads) for idx in selected_idx]\n",
    "print(f\"Selected Heads (L-H): {selected_heads}\")\n",
    "\n",
    "\n",
    "# Set style for better aesthetics\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "\n",
    "# Sorted diff curve and TOPK vertical lines\n",
    "diff_flat = diff.flatten().cpu().numpy()\n",
    "sorted_vals = np.sort(diff_flat)[::-1]  # descending\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Plot sorted diff curve\n",
    "ax.plot(sorted_vals, marker='o', linestyle='-', alpha=0.7, \n",
    "        linewidth=2, markersize=3, color='#2E86AB', label='Diff values')\n",
    "ax.axvline(x=TOP_HEADS - 0.5, color='#E63946', linestyle='--', \n",
    "           linewidth=2.5, label=f'Top {TOP_HEADS} boundary', alpha=0.8)\n",
    "ax.axvline(x=len(sorted_vals) - TOP_HEADS - 0.5, color='#F77F00', \n",
    "           linestyle='--', linewidth=2.5, label=f'Bottom {TOP_HEADS} boundary', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Head Rank (sorted by diff, descending)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Diff (param ‚àí context)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Sorted Difference Curve with Top-K Boundaries', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(frameon=True, shadow=True, fontsize=10)\n",
    "ax.grid(True, alpha=0.3, linestyle=':', linewidth=1)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3570ff6da7a83449"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By plotting the contrastive scores, we see a physical separation between the two types of knowledge. Parametric Heads (Red) tend to cluster in the very early and late layers, suggesting they are involved in initial processing and final output generation. In contrast, In-Context Heads (Blue) dominate the middle-to-late layers, where the model performs complex reasoning and integration of the provided text.",
   "id": "4b4920eab9693cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot atlas for In-Context and Parametric Heads\n",
    "\n",
    "# Let's take only positive relevances into account to focus on supportive heads\n",
    "diff = param_relevance.clamp(min=0).mean(dim=0) - context_relevance.clamp(min=0).mean(dim=0)\n",
    "\n",
    "# Prepare Data for DataFrame\n",
    "diff_scores = diff.cpu()\n",
    "num_layers, num_heads = diff_scores.shape\n",
    "layers = np.repeat(np.arange(num_layers), num_heads)\n",
    "heads = np.tile(np.arange(num_heads), num_layers)\n",
    "\n",
    "# Flatten scores\n",
    "raw_scores = diff_scores.flatten().numpy()\n",
    "\n",
    "# Determine Head Type (Positive = In-Context, Negative = Parametric)\n",
    "head_types = np.where(raw_scores > 0, 'Parametric', 'In-Context')\n",
    "\n",
    "# Take Magnitudes (Absolute value)\n",
    "magnitudes = np.abs(raw_scores)\n",
    "\n",
    "# 5. Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'layer': layers,\n",
    "    'head': heads,\n",
    "    'score': magnitudes,     # Magnitude for dot size\n",
    "    'type': head_types  # Categorical for color\n",
    "})\n",
    "plot_atlas(df, color_col='type', size_col='score',top_k=80)"
   ],
   "id": "1fa62b2cee56ebb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Monitoring System",
   "id": "c38629d065b818bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We train a Linear Discriminant Analysis (LDA) classifier to automatically detect if a head is behaving \"parametrically\" or \"contextually.\"",
   "id": "abb248c046827910"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train LDA classifier on provided \"parametric\" (true_mean) and \"contextual\" (wrong_mean) examples\n",
    "X_param = param_relevance.reshape(param_relevance.shape[0], -1)[:, selected_idx].detach().cpu().numpy()\n",
    "X_ctx = context_relevance.reshape(context_relevance.shape[0], -1)[:, selected_idx].detach().cpu().numpy()\n",
    "\n",
    "X_train = np.concatenate([X_param, X_ctx])\n",
    "y_train = np.concatenate([np.ones(X_param.shape[0], dtype=int), np.zeros(X_ctx.shape[0], dtype=int)])\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, labels shape: {y_train.shape}\")\n",
    "print(f\"Class counts: parametric={int(y_train.sum())}, contextual={int((y_train==0).sum())}\")\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "correct = 0\n",
    "for train_idx, test_idx in loo.split(X_train):\n",
    "    lda.fit(X_train[train_idx], y_train[train_idx])\n",
    "    pred = lda.predict(X_train[test_idx])[0]\n",
    "    correct += int(pred == y_train[test_idx][0])\n",
    "\n",
    "loo_accuracy = correct / len(X_train)\n",
    "print(f\"  LDA Leave-One-Out Accuracy: {loo_accuracy:.3f}\\n\")\n",
    "\n",
    "# Train on full training data\n",
    "lda.fit(X_train, y_train)\n",
    "print(\"LDA classifier trained on full dataset\")\n"
   ],
   "id": "9e4674d17a1c613d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We generate a medical report and use our trained LDA classifier to analyze every single predicted token. This determines if the information came from the context (the patient record) or the model's internal memory (potential hallucination).",
   "id": "e64339344de41d69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apply to Medical Text - TOKEN LEVEL\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PART 2: CLASSIFYING GENERATED MEDICAL TEXT - TOKEN BY TOKEN\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "record = \"\"\"\n",
    "PATIENT RECORD - John Smith, Age 45\n",
    "\n",
    "Chief Complaint: Chest pain, shortness of breath\n",
    "History: Hypertension (diagnosed 2019), smoker (20 years)\n",
    "Current Medications: Lisinopril 10mg daily\n",
    "Vitals: BP 145/95, HR 88, Temp 98.6¬∞F\n",
    "Lab Results: Pending\n",
    "Physical Exam: Mild chest discomfort on palpation\n",
    "Patient Reports: Pain started 3 hours ago, radiating to left arm\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a medical assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{record}\\n\\nWrite a comprehensive handover summary for the ward team. Include what to monitor and why.\"}\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating medical handover summary...\")\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, return_tensors=\"pt\", tokenize=True, \n",
    "    add_special_tokens=True, add_generation_prompt=True\n",
    ").to(model.device)\n",
    "input_length = input_ids.shape[1]\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "full_sequence = output_ids[0]\n",
    "generated_tokens = full_sequence[input_length:]\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"GENERATED TEXT:\")\n",
    "print(\"-\"*100)\n",
    "print(generated_text)\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "# TOKEN-LEVEL CLASSIFICATION\n",
    "def classify_tokens(model, input_ids, generated_tokens):\n",
    "    \"\"\"Compute attention relevance for a specific token position\"\"\"\n",
    "\n",
    "    token_classifications = []\n",
    "\n",
    "    input_ids = input_ids.clone()\n",
    "    embeds = model.get_input_embeddings()(input_ids.unsqueeze(0))\n",
    "    embeds.requires_grad = True\n",
    "\n",
    "    outputs = model(inputs_embeds=embeds, use_cache=False, output_attentions=True)\n",
    "\n",
    "    for attn in outputs.attentions:\n",
    "        attn.retain_grad()\n",
    "\n",
    "    print(f\"\\nClassifying {len(generated_tokens)} tokens...\\n\")\n",
    "\n",
    "    for i in tqdm(range(len(generated_tokens))):\n",
    "        token_id = generated_tokens[i].item()\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        abs_position = input_length + i\n",
    "        \n",
    "        if abs_position >= len(full_sequence):\n",
    "            continue\n",
    "        \n",
    "        # Target the logit for this specific token at this position\n",
    "        target_token_id = input_ids[abs_position]\n",
    "        target_logit = outputs.logits[0, abs_position - 1, target_token_id]\n",
    "\n",
    "        model.zero_grad()\n",
    "        for attn in outputs.attentions:\n",
    "            if attn.grad is not None:\n",
    "                attn.grad.zero_()\n",
    "\n",
    "        target_logit.backward(retain_graph=True)\n",
    "        \n",
    "        relevances = []\n",
    "        for l_idx, h_idx in selected_heads:\n",
    "            attn = outputs.attentions[l_idx]\n",
    "            rel = (attn[0, h_idx] * attn.grad[0, h_idx]).float().sum(dim=[-2, -1]).item()\n",
    "            relevances.append(rel)\n",
    "\n",
    "        features = np.array(relevances).reshape(1, -1)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = lda.predict_proba(features)[0]\n",
    "        prob_contextual = probabilities[0]  # Class 0\n",
    "        prob_parametric = probabilities[1]  # Class 1\n",
    "        \n",
    "        # Classify\n",
    "        prediction = lda.predict(features)[0]\n",
    "        class_label = 'parametric' if prediction == 1 else 'contextual'\n",
    "        \n",
    "        token_classifications.append({\n",
    "            'token': token_text,\n",
    "            'token_id': token_id,\n",
    "            'position': i,\n",
    "            'prob_contextual': prob_contextual,\n",
    "            'prob_parametric': prob_parametric,\n",
    "            'class': class_label\n",
    "        })\n",
    "\n",
    "    return token_classifications\n",
    "\n",
    "token_classifications = classify_tokens(model, full_sequence, generated_tokens)"
   ],
   "id": "88d6b4f90b237891"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# COLOR-CODED VISUALIZATION (contextual=blue, parametric=red)\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"ORIGINAL PATIENT RECORD:\")\n",
    "print(\"-\"*100)\n",
    "print(record)\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "def get_color_for_prob(prob_parametric):\n",
    "    \"\"\"\n",
    "    Return ANSI color code based on parametric probability.\n",
    "    Red (parametric) -> Yellow (mixed) -> Blue (contextual)\n",
    "    \"\"\"\n",
    "    if prob_parametric > 0.7:\n",
    "        return \"\\033[31m\"  # Bright red / strong parametric\n",
    "    elif prob_parametric > 0.6:\n",
    "        return \"\\033[91m\"  # Red\n",
    "    elif prob_parametric > 0.5:\n",
    "        return \"\\033[93m\"  # Yellow (mixed)\n",
    "    elif prob_parametric > 0.4:\n",
    "        return \"\\033[33m\"  # Dark yellow (mixed)\n",
    "    elif prob_parametric > 0.3:\n",
    "        return \"\\033[94m\"  # Bright blue (contextual-leaning)\n",
    "    else:\n",
    "        return \"\\033[34m\"  # Blue (strongly contextual)\n",
    "\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"TOKEN-LEVEL CLASSIFICATION WITH COLOR CODING\")\n",
    "print(\"=\"*120)\n",
    "print(f\"Classifier: Linear Discriminant Analysis (LDA)\")\n",
    "print(f\"Training LOO Accuracy: {loo_accuracy:.3f}\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\nColor Legend:\")\n",
    "print(f\"\\033[31m‚ñà‚ñà\\033[0m Bright Red: Strongly Parametric (P > 0.7)\")\n",
    "print(f\"\\033[91m‚ñà‚ñà\\033[0m Red: Parametric (P > 0.6)\")\n",
    "print(f\"\\033[93m‚ñà‚ñà\\033[0m Yellow: Mixed (0.4 < P ‚â§ 0.6)\")\n",
    "print(f\"\\033[94m‚ñà‚ñà\\033[0m Bright Blue: Contextual-leaning (P > 0.3)\")\n",
    "print(f\"\\033[34m‚ñà‚ñà\\033[0m Blue: Strongly Contextual (P ‚â§ 0.3)\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\nCOLOR-CODED TEXT:\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "# Print with colors\n",
    "colored_text = \"\"\n",
    "for tc in token_classifications:\n",
    "    color = get_color_for_prob(tc['prob_parametric'])\n",
    "    colored_text += f\"{color}{tc['token']}{RESET}\"\n",
    "\n",
    "print(colored_text)\n",
    "print(\"-\"*120)"
   ],
   "id": "807a63b4624ec512"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
